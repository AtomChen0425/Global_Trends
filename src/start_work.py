#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
"""

import os
import logging
import concurrent.futures
from datetime import datetime
from typing import Dict, List

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

HN_AVAILABLE = False
GITHUB_AVAILABLE = False
ARXIV_AVAILABLE = False
AI_AVAILABLE = False

try:
    from FetchPipeline.HN_pipeline import fetch_top_stories
    HN_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Hacker News pipeline ä¸å¯ç”¨ï¼Œè·³è¿‡ã€‚åŽŸå› : {e}")

try:
    from FetchPipeline.github_pipeline import fetch_github_from_web, enrich_trend_data
    GITHUB_AVAILABLE = True
except ImportError as e:
    logger.warning(f"GitHub pipeline ä¸å¯ç”¨ï¼Œè·³è¿‡ã€‚åŽŸå› : {e}")

try:
    from FetchPipeline.arxiv_pipeline import fetch_papers_by_category
    ARXIV_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ArXiv pipeline ä¸å¯ç”¨ï¼Œè·³è¿‡ã€‚åŽŸå› : {e}")

try:
    from utils.AI_Agent import gemini_summarize
    AI_AVAILABLE = True
except ImportError as e:
    logger.error(f"AI ä»£ç†æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†æ— æ³•ç”Ÿæˆåˆ†æžæŠ¥å‘Šï¼åŽŸå› : {e}")

FETCH_TIMEOUT = 60  # å•ä¸ªæ•°æ®æºçš„æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

def _fetch_hn(limit: int) -> List[Dict]:
    """æŠ“å– Hacker News æ•°æ®"""
    if not HN_AVAILABLE: return []
    logger.info("å¼€å§‹æŠ“å– Hacker News...")
    results = []
    try:
        stories = fetch_top_stories(limit=limit)
        for s in stories:
            results.append({
                "source": "Hacker News",
                "title": s.title,
                "url": s.url or s.hn_url,
                "score": s.score
            })
        logger.info(f"Hacker News: æˆåŠŸæŠ“å– {len(results)} æ¡")
    except Exception as e:
        logger.warning(f"Hacker News æŠ“å–å¤±è´¥: {e}")
    return results

def _fetch_github(limit: int) -> List[Dict]:
    """æŠ“å– GitHub Trending æ•°æ®"""
    if not GITHUB_AVAILABLE: return []
    logger.info("å¼€å§‹æŠ“å– GitHub Trending...")
    results = []
    try:
        trends_raw = fetch_github_from_web()
        # æˆªå–å‰ N ä¸ªå¹¶èŽ·å– README ç­‰è¯¦ç»†ä¿¡æ¯
        trends = enrich_trend_data(trends_raw[:limit])
        for t in trends:
            results.append({
                "source": "GitHub å¼€æºé¡¹ç›®",
                "title": t.name,
                "url": t.url,
                "description": t.description,
                "stars": t.stars
            })
        logger.info(f"GitHub: æˆåŠŸæŠ“å– {len(results)} æ¡")
    except Exception as e:
        logger.warning(f"GitHub æŠ“å–å¤±è´¥: {e}")
    return results

def _fetch_arxiv(category: str, limit: int) -> List[Dict]:
    """æŠ“å– ArXiv æœ€æ–°è®ºæ–‡æ•°æ®"""
    if not ARXIV_AVAILABLE: return []
    logger.info(f"å¼€å§‹æŠ“å– ArXiv ({category})...")
    results = []
    try:
        papers = fetch_papers_by_category(category, limit=limit)
        for p in papers:
            results.append({
                "source": "ArXiv å­¦æœ¯è®ºæ–‡",
                "title": p.title,
                "url": p.url,
                "authors": ", ".join(p.authors),
                "summary": p.summary[:400] 
            })
        logger.info(f"ArXiv: æˆåŠŸæŠ“å– {len(results)} æ¡")
    except Exception as e:
        logger.warning(f"ArXiv æŠ“å–å¤±è´¥: {e}")
    return results


def fetch_all_sources(hn_limit=10, gh_limit=10, arxiv_limit=10) -> Dict[str, List[Dict]]:
    """å¹¶è¡ŒæŠ“å–æ‰€æœ‰é…ç½®çš„æ•°æ®æº"""
    logger.info("å¯åŠ¨æ•°æ®æŠ“å–å¼•æ“Ž...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        # å¹¶è¡Œæäº¤ä»»åŠ¡
        future_hn = executor.submit(_fetch_hn, hn_limit)
        future_gh = executor.submit(_fetch_github, gh_limit)
        future_arxiv = executor.submit(_fetch_arxiv, "cs.CV", arxiv_limit)

        def _safe_result(future, name):
            try:
                return future.result(timeout=FETCH_TIMEOUT)
            except concurrent.futures.TimeoutError:
                logger.warning(f"[{name}] æ•°æ®æºæŠ“å–è¶…æ—¶ ({FETCH_TIMEOUT}s)")
                return []
            except Exception as e:
                logger.warning(f"[{name}] æ•°æ®æºæŠ“å–å¼‚å¸¸: {e}")
                return []

        # æ”¶é›†ç»“æžœ
        intel = {
            "hn": _safe_result(future_hn, "Hacker News"),
            "github": _safe_result(future_gh, "GitHub"),
            "arxiv": _safe_result(future_arxiv, "ArXiv"),
        }
        
    total_items = sum(len(v) for v in intel.values())
    logger.info(f"âœ… All done! æ”¶é›†åˆ°{total_items} æ¡ã€‚")
    return intel

def format_intel_for_ai(intel: Dict[str, List[Dict]]) -> str:
    formatted_text = ""
    
    for item in intel.get("hn", []):
        formatted_text += f"[æ¥æº: {item['source']}]\næ ‡é¢˜: {item['title']}\né“¾æŽ¥: {item['url']}\nçƒ­åº¦: {item['score']} points\n---\n"
        
    for item in intel.get("github", []):
        formatted_text += f"[æ¥æº: {item['source']}]\næ ‡é¢˜: {item['title']}\né“¾æŽ¥: {item['url']}\næè¿°: {item.get('description', '')}\nStaræ•°: {item['stars']}\n---\n"
        
    for item in intel.get("arxiv", []):
        formatted_text += f"[æ¥æº: {item['source']}]\næ ‡é¢˜: {item['title']}\nä½œè€…: {item['authors']}\né“¾æŽ¥: {item['url']}\næ‘˜è¦: {item['summary']}...\n---\n"
        
    return formatted_text

def main():
    # 1. å¹¶è¡Œæ”¶é›†æ•°æ®
    intel_data = fetch_all_sources(hn_limit=5, gh_limit=3, arxiv_limit=3)
    raw_data_string = format_intel_for_ai(intel_data)
    
    if not raw_data_string.strip():
        logger.error("æœªæ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # 2. è°ƒç”¨ AI è¿›è¡Œåˆ†æž
    if not AI_AVAILABLE:
        logger.error("AI æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚åŽŸå§‹æ•°æ®:\n" + raw_data_string)
        return

    logger.info("ðŸ§  æ­£åœ¨å°†åŽŸå§‹æƒ…æŠ¥æäº¤ç»™å¤§æ¨¡åž‹è¿›è¡Œæ·±åº¦åˆ†æž (å¯èƒ½éœ€è¦ 15-30 ç§’)...")
    try:
        final_report = gemini_summarize(raw_data_string)
        if not final_report:
            raise ValueError("AI è¿”å›žå†…å®¹ä¸ºç©º")
    except Exception as e:
        logger.error(f"AI ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

    # 3. å­˜å‚¨æŠ¥å‘Š
    output_dir = "reports"
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(output_dir, f"Intel_Report_{timestamp}.md")
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(final_report)
        
    logger.info(f"ðŸŽ‰ æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜è‡³ -> {filename}")

if __name__ == "__main__":
    intel= fetch_all_sources()
    # print(intel)
    print(format_intel_for_ai(intel))